{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define your query\n",
    "query = \"\"\"\n",
    "WITH sparse_feature AS (\n",
    "  SELECT\n",
    "    review_number,\n",
    "    review,\n",
    "    STRING_AGG(CAST(word_index AS STRING), ', ') AS feature,\n",
    "    label,\n",
    "    split\n",
    "  FROM (\n",
    "    SELECT\n",
    "      DISTINCT review_number,\n",
    "      review,\n",
    "      word,\n",
    "      label,\n",
    "      split\n",
    "    FROM\n",
    "      sparse_features_demo.processed_reviews,\n",
    "      UNNEST(words) AS word\n",
    "    WHERE\n",
    "      word IN (SELECT word FROM sparse_features_demo.vocabulary)\n",
    "  ) AS word_list\n",
    "  LEFT JOIN\n",
    "    sparse_features_demo.vocabulary AS topk_words\n",
    "    ON word_list.word = topk_words.word\n",
    "  GROUP BY\n",
    "    review_number,\n",
    "    review,\n",
    "    label,\n",
    "    split\n",
    ")\n",
    "SELECT review, feature, label FROM sparse_feature\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(query).to_dataframe()\n",
    "\n",
    "\n",
    "def convert_to_labels(text):\n",
    "    return 0 if 'Negative' in text else 1\n",
    "\n",
    "\n",
    "df['label'] = df.label.apply(convert_to_labels)\n",
    "# Convert the 'feature' column from a comma-separated string to a list of integers\n",
    "df['feature'] = df['feature'].apply(lambda x: list(map(int, x.split(', '))))\n",
    "\n",
    "# Ensure all sequences are of max_length by padding\n",
    "max_length = 100  # Define your max sequence length\n",
    "df['feature'] = df['feature'].apply(lambda x: x + [0] * (max_length - len(x)) if len(x) < max_length else x[:max_length])\n",
    "\n",
    "X = df['feature'].tolist()\n",
    "y = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,  test_size=0.2, random_state=42)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\volte\\Documents\\gcp_practice\\GCP_functions_and_scripts\\tf_on_gcp_experiments\\.env\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 30000\n",
    "max_length = 100\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(max_length,)),\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is :  <_BatchDataset element_spec=(TensorSpec(shape=(None, 100), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 172ms/step - accuracy: 0.6713 - loss: 0.5534 - val_accuracy: 0.9151 - val_loss: 0.2280\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create trainnig dataset\n",
    "batch_size= 100\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train), tf.convert_to_tensor(y_train)))\n",
    "train_ds = train_ds.shuffle(buffer_size=len(X_train)).batch(batch_size)\n",
    "print('Input is : ', train_ds)\n",
    "\n",
    "# Create validation dataset\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_val), tf.convert_to_tensor(y_val)))\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m199/199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9130 - loss: 0.2318\n",
      "\n",
      "Test accuracy: 0.9164528846740723\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_test), tf.convert_to_tensor(y_test)))\n",
    "test_ds = test_ds.batch(batch_size)\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print('\\nTest accuracy: {}'.format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_4/assets\n"
     ]
    }
   ],
   "source": [
    "#model.save('model_4.h5')\n",
    "tf.saved_model.save(model, 'model_4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start upload\n",
      "Uploaded .\\model_4.h5 to model_4.h5\n",
      "Uploaded .\\.env\\Lib\\site-packages\\h5py\\tests\\data_files\\vlen_string_dset.h5 to .env\\Lib\\site-packages\\h5py\\tests\\data_files\\vlen_string_dset.h5\n",
      "Uploaded .\\.env\\Lib\\site-packages\\h5py\\tests\\data_files\\vlen_string_dset_utc.h5 to .env\\Lib\\site-packages\\h5py\\tests\\data_files\\vlen_string_dset_utc.h5\n",
      "Uploaded .\\.env\\Lib\\site-packages\\h5py\\tests\\data_files\\vlen_string_s390x.h5 to .env\\Lib\\site-packages\\h5py\\tests\\data_files\\vlen_string_s390x.h5\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "def upload_to_gcs(bucket_name, source_folder, destination_blob_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    for root, _, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            if '.h5' in file:\n",
    "                local_path = os.path.join(root, file)\n",
    "                blob_path = os.path.relpath(local_path, source_folder)\n",
    "                blob = bucket.blob(os.path.join(destination_blob_name, blob_path))\n",
    "                blob.upload_from_filename(local_path)\n",
    "                print(f\"Uploaded {local_path} to {blob_path}\")\n",
    "\n",
    "bucket_name = 'reddit_raw_data_0184598608709384596'\n",
    "source_folder = '.'\n",
    "destination_blob_name = 'model_v4/'\n",
    "print('start upload')\n",
    "\n",
    "upload_to_gcs(bucket_name, source_folder, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model5\\assets\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=model_5/. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(model_5/, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model_load_test \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_5/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#model_load_test = tf.saved_model.load('model_4')\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#model_load_test = keras.saving.load_model('model_4.h5')\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\gcp_practice\\GCP_functions_and_scripts\\tf_on_gcp_experiments\\.env\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:193\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy H5 format files (`.h5` extension). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that the legacy SavedModel format is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by `load_model()` in Keras 3. In \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder to reload a TensorFlow SavedModel as an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference-only layer in Keras 3, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.layers.TFSMLayer(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, call_endpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note that your `call_endpoint` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File format not supported: filepath=model_5/. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(model_5/, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, './model5')\n",
    "model_load_test = tf.keras.models.load_model('model_5/')\n",
    "#model_load_test = tf.saved_model.load('model_4')\n",
    "#model_load_test = keras.saving.load_model('model_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#model_load_test.__dict__\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel_load_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(X_test)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "#model_load_test.__dict__\n",
    "model_load_test.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
